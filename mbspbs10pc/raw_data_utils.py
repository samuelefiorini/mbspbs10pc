"""This module keeps the functions for sequences extraction from MBS files."""
from __future__ import print_function

import datetime
import multiprocessing as mp
import os
import warnings
from multiprocessing import Manager

import numpy as np
import pandas as pd
from mbspbs10pc.concessionals_utils import flatten
from pandas.core.common import SettingWithCopyWarning
from tqdm import tqdm

___MBS_FILES_DICT__ = dict()


def worker(i, split, raw_data):
    """Patient tracking worker."""
    mbs_dd = ___MBS_FILES_DICT__  # nice nickname for the ugly global variable

    with warnings.catch_warnings():  # ignore SettingWithCopyWarning
        warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)

        # First progress
        progress = tqdm(
            total=len(mbs_dd.keys()),
            position=i,
            desc="[job {}] Pre-filtering".format(i),
        )

        # Pre-filter: keep only the elements of mbs_dd that are in the current split
        # this helps in reducing the time of the next step
        small_mbs_dd = dict()
        # for k in tqdm(sorted(mbs_dd.keys()), desc='[job {}] Pre-filtering'.format(i)):
        for k in sorted(mbs_dd.keys()):
            progress.update(1)
            # keep only a subset of the full MBS data
            small_mbs_dd[k] = mbs_dd[k].loc[mbs_dd[k]['PIN'].isin(split)]
            # change format to the right datetime format (this is gonna be useful later)
            small_mbs_dd[k].loc[:, 'DOS'] = pd.to_datetime(small_mbs_dd[k]['DOS'], format='%d%b%Y')
            # and sort by date
            small_mbs_dd[k].sort_values(by='DOS', inplace=True)
        progress.close()

        # Second progress
        progress = tqdm(
            total=len(split),
            position=i,
            desc="[job {}] Sequence extraction".format(i),
        )

        # Now track down each patient in the reduced MBS files
        # for s in tqdm(split, desc='[job {}] Sequence extraction'.format(i)):
        for s in split:
            progress.update(1)
            tmp = pd.DataFrame(columns=['PIN', 'DOS', 'BTOS'])
            for k in sorted(mbs_dd.keys()):
                tmp = pd.concat((tmp, small_mbs_dd[k].loc[small_mbs_dd[k]['PIN'] == s]))

            if len(tmp['BTOS'].values) > 0:
                # evaluate the first order difference and convert each entry in days
                timedeltas = map(lambda x: pd.Timedelta(x).days,
                                 tmp['DOS'].values[1:] - tmp['DOS'].values[:-1])
                # then build the sequence as ['exam', idle-days, 'exam', idle-days, ...]
                raw_data[s] = flatten([[btos, dt] for btos, dt in zip(tmp['BTOS'].values, timedeltas)])
                raw_data[s].append(tmp['BTOS'].values[-1])
            else:
                raw_data[s] = list()
        progress.close()


def get_raw_data(mbs_files, sample_pin_lookout, source, n_jobs=4):
    """Extract the sequences and find the additional features.

    This function, given the input list of patient identifiers `source` scans
    the `mbs_files` and extract the MBS items sequence. It also finds additional
    info on the patient such as age and sex from the `sample_pin_lookout` file.
    Unstructured sequences and additional info are referred to as raw_data.

    Parameters:
    --------------
    mbs_files: list
        List of input MBS files.

    sample_pin_lookout: string
        Location of the `SAMPLE_PIN_LOOKUP.csv` file.

    source: string
        Location of the 'PTNT_ID' csv file generated by `labels_assignment.py`.

    n_jobs: integer
        The number of processes that have asyncronous access to the input files.

    Returns:
    --------------
    raw_data: dictionary
        A dictionary that stores raw unstructured sequences of each input
        subject and additional info.

    extra_info: pandas.DataFrame
        Extra info of the input patients, such as age and sex.
    """
    raw_data = dict()

    # Step 0: load the source file and the imap file
    dfs = pd.read_csv(source, header=0)
    imap = pd.read_csv(os.path.join('data', 'imap.tsv'), sep='\t', header=0,
                       usecols=['ITEM', 'BTOS'])

    # Step 1: get sex and age
    df_pin_lookout = pd.read_csv(sample_pin_lookout, header=0)
    df_pin_lookout['AGE'] = datetime.datetime.now().year - df_pin_lookout['YOB']
    dfs = pd.merge(dfs, df_pin_lookout, how='left', left_on='PTNT_ID', right_on='PIN')[['PIN', 'SEX', 'AGE']]

    # Step 2: follow each patient in the mbs files
    # at first create a very large dictionary with all the MBS files
    # (keeping only the relevant columns)
    global ___MBS_FILES_DICT__
    for mbs in tqdm(mbs_files, desc='MBS files loading'):
        dd = pd.read_csv(mbs, header=0, usecols=['PIN', 'ITEM', 'DOS'])
        ___MBS_FILES_DICT__[mbs] = pd.merge(dd, imap, how='left', on='ITEM')

    # This large dictionary is shared across multiple processes
    manager = Manager()
    shared_raw_data = manager.dict(raw_data)
    pool = mp.Pool(n_jobs)

    # Split the PINs in n_jobs approximately equal chunks
    splits = np.array_split(dfs['PIN'].values, n_jobs)

    # Submit the patient tracking jobs
    results = [pool.apply_async(worker, (i, split, shared_raw_data)) for i, split in enumerate(splits)]

    # And collect the results
    results = [p.get() for p in results]

    # Clear screen
    if os.name == 'posix':
        os.system('clear')
    else:
        os.system('cls')

    return dict(shared_raw_data), dfs
