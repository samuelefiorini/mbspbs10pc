"""This module keeps the functions for sequences extraction from MBS files."""
from __future__ import print_function

import calendar
import datetime
import multiprocessing as mp
import os
from multiprocessing import Manager

import numpy as np
import pandas as pd
from sklearn.datasets.base import Bunch
from tqdm import tqdm

___MBS_FILES_DICT__ = dict()


def worker(pins, raw_data):
    """Patient tracking worker."""
    for pin in pins:
        raw_data[pin] = len(___MBS_FILES_DICT__)


def get_raw_data(mbs_files, sample_pin_lookout, source, n_jobs=4):
    """Extract the sequences and find the additional features.

    This function, given the input list of patient identifiers `source` scans
    the `mbs_files` and extract the MBS items sequence. It also finds additional
    info on the patient such as age and sex from the `sample_pin_lookout` file.
    Unstructured sequences and additional info are referred to as raw_data.

    Parameters:
    --------------
    mbs_files: list
        List of input MBS files.

    sample_pin_lookout: string
        Location of the `SAMPLE_PIN_LOOKUP.csv` file.

    source: string
        Location of the 'PTNT_ID' csv file generated by `labels_assignment.py`.

    n_jobs: integer
        The number of processes that have asyncronous access to the input files.

    Returns:
    --------------
    raw_data: dictionary
        A dictionary that stores raw unstructured sequences and additional info
        of each input subject. Each value in the dictionary is a
        `sklearn.datasets.base.Bunch` object.
    """
    raw_data = dict()

    # Step 0: load the source file
    dfs = pd.read_csv(source, header=0)

    # Step 1: get sex and age
    df_pin_lookout = pd.read_csv(sample_pin_lookout, header=0)
    df_pin_lookout['AGE'] = datetime.datetime.now().year - df_pin_lookout['YOB']
    dfs = pd.merge(dfs, df_pin_lookout, how='left', left_on='PTNT_ID', right_on='PIN')[['PIN', 'SEX', 'AGE']]

    # Step 2: follow each patient in the mbs files
    # at first create a very large dictionary with all the MBS files
    # (keeping only the relevant columns)
    print('* Loading the MBS files ...')
    global ___MBS_FILES_DICT__
    for mbs in tqdm(mbs_files):
        ___MBS_FILES_DICT__[mbs] = pd.read_csv(mbs, header=0,
                                               usecols=['PIN', 'SPR_RSP', 'DOS'])
    print('* {} MBS files loaded'.format(len(___MBS_FILES_DICT__)))

    # This large dictionary is shared across multiple processes
    manager = Manager()
    shared_raw_data = manager.dict(raw_data)
    pool = mp.Pool(n_jobs)

    # Split the PINs in n_jobs approximately equal chunks
    splits = np.array_split(dfs['PIN'].values, n_jobs)

    # Submit the patient tracking jobs
    results = [pool.apply_async(worker, (split, shared_raw_data)) for split in splits]

    # And collect the results
    results = [p.get() for p in results]

    return dict(shared_raw_data)
