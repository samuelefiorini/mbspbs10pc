#!/usr/bin/env python
"""Train bidirectional timestamp-guided attention model.

Train the keras model on the data extracted by `extract_sequences.py` and
matched by CEM (see `matching_step1.py` and `matching_step2.R`).
"""

from __future__ import print_function

import argparse
import os
import warnings
from datetime import datetime
import matplotlib
matplotlib.use('Agg')
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import pandas as pd
from keras import optimizers as opt
from keras.callbacks import (EarlyStopping, History, ModelCheckpoint,
                             TensorBoard, ReduceLROnPlateau)
from keras.layers import CuDNNLSTM
from keras.utils import plot_model
from mbspbs10pc.model import build_model
from mbspbs10pc.plotting import plot_confusion_matrix, plot_history
from mbspbs10pc.utils import (load_data_labels, tokenize,
                              train_validation_test_split)
from sklearn import metrics


def parse_arguments():
    """"Parse input arguments."""
    parser = argparse.ArgumentParser(description='MBS-PBS 10% data/labels '
                                                 'extraction.')
    parser.add_argument('-l', '--labels', type=str,
                        help='Path to the labels csv file generated by '
                             '`matching_step2.R`.',
                        default=None)
    parser.add_argument('-d', '--data', type=str,
                        help='Path to the data pkl file generated by '
                        '`extract_sequences.py`.',
                        default=None)
    parser.add_argument('-e', '--embedding', type=str,
                        help='Path to the embedding matrix csv file.',
                        default=None)
    parser.add_argument('-o', '--output', type=str,
                        help='Ouput file name root.',
                        default=None)
    args = parser.parse_args()
    return args


def init_main():
    """Initialize the main routine."""
    args = parse_arguments()

    if args.labels is None or not os.path.exists(args.labels):
        raise ValueError('{} labels file not found'.format(args.labels))

    if args.data is None or not os.path.exists(args.data):
        raise ValueError('{} data file not found'.format(args.data))

    # Check output filename
    if args.output is None:
        args.output = 'out_' + str(datetime.now())

    return args


def get_callbacks(RLRP_patience=7, ES_patience=15, MC_filepath=None):
    """Get the callbacks list.

    Parameters:
    --------------
    RLRP_patience: int (default=7)
        Patience for ReduceLROnPlateau.

    ES_patience: int (default=15)
        Patience for EarlyStopping.

    MC_filepath: string
        ModelCheckpoint filepath.

    Returns:
    --------------
    callbacks: list
        Callbacks list:
        [ReduceLROnPlateau(...), EarlyStopping(...), ModelCheckpoint(...)]
    """
    callbacks = [ReduceLROnPlateau(monitor='val_loss',
                                   factor=0.5, patience=RLRP_patience,
                                   min_lr=1e-6, verbose=1),
                 EarlyStopping(monitor='val_loss', patience=ES_patience),
                 ModelCheckpoint(filepath=MC_filepath+'_weights.h5',
                                 save_best_only=True, save_weights_only=True),
                 TensorBoard(log_dir='/tmp/logs', histogram_freq=3,
                             batch_size=128, write_graph=True,
                             embeddings_freq=3,
                             embeddings_layer_names=[])]
    return callbacks


def concatenate_history(h):
    """Concatenate History objects.

    Useful for layers fine tuning.

    Parameters:
    --------------
    h: list
        List of history objects to concatenate.

    Returns:
    --------------
    history: `keras.callbacks.History`
        The resulting `History` object.
    """
    h0, h1 = h[0], h[1]
    history = History()
    history.history = {}
    history.epoch = h0.epoch + h1.epoch

    for k in h0.history.keys():
        history.history[k] = h0.history[k] + h1.history[k]

    return history


def fit_model(model, training_set, validation_set, outputfile,
              fine_tune_embedding=False):
    # Start training
    print('* Training model...')
    callbacks = get_callbacks(RLRP_patience=3, ES_patience=12,
                              MC_filepath=outputfile)

    history = model.fit(x=training_set[0], y=training_set[1],
                        epochs=100,
                        callbacks=callbacks,
                        batch_size=128,
                        validation_data=validation_set)

    if fine_tune_embedding:
        print(print('* Fine-tuning the embedding layer...'))
        # Fine-tune the embedding layers
        model.get_layer('mbs_embedding').trainable = True

        # Re-compile the model
        model.compile(optimizer=opt.RMSprop(lr=0.001),
                      loss='binary_crossentropy',
                      metrics=['acc'])

        callbacks = get_callbacks(RLRP_patience=5, ES_patience=10,
                                  MC_filepath=outputfile+'_finetuned')

        history_ft = model.fit(x=training_set[0], y=training_set[1],
                               epochs=200,
                               callbacks=callbacks,
                               batch_size=128,
                               validation_data=validation_set,
                               initial_epoch=history.epoch[-1])
        history = concatenate_history([history, history_ft])

    print('* Saving training history...', end=' ')
    plt.figure(dpi=100)
    plot_history(history)
    plt.savefig(outputfile+'_loss_history.png')
    print(u'\u2713')
    return model


def main():
    """Main train.py routine."""
    print('-------------------------------------------------------------------')
    print('MBS - PBS 10% dataset utility: train.py')
    print('-------------------------------------------------------------------')
    args = init_main()

    # Load data
    print('* Loading {} and {}...'.format(args.data, args.labels), end=' ')
    dataset = load_data_labels(args.data, args.labels)
    print(u'\u2713')

    # Load embedding matrix
    print('* Loading {}...'.format(args.embedding), end=' ')
    embedding_matrix = pd.read_csv(
        args.embedding, header=0, index_col=0).values
    print(u'\u2713')

    # Tokenize and pad
    print('* Preparing data...', end=' ')
    padded_mbs_seq, padded_timestamp_seq, _ = tokenize(dataset)
    maxlen = padded_mbs_seq.shape[1]

    # Split in training, validation, test sets
    tr_set, v_set, ts_set = train_validation_test_split(
        [padded_mbs_seq, padded_timestamp_seq], dataset['Class'],
        test_size=0.4, validation_size=0.1,
        verbose=False)
    print(u'\u2713')

    # Build the model
    print('* Building model...', end=' ')
    model = build_model(mbs_input_shape=(maxlen,),
                        timestamp_input_shape=(maxlen, 1),
                        vocabulary_size=embedding_matrix.shape[0],
                        embedding_size=embedding_matrix.shape[1],
                        recurrent_units=64,
                        dense_units=512,
                        bidirectional=True,
                        LSTMLayer=CuDNNLSTM)

    # Initialize the embedding matrix
    model.get_layer('mbs_embedding').set_weights([embedding_matrix])
    model.get_layer('mbs_embedding').trainable = True

    # Compile the model
    model.compile(optimizer=opt.RMSprop(lr=0.004),
                  loss='binary_crossentropy',
                  metrics=['acc'])
    print(u'\u2713')

    # Print the summary to file
    print('* Saving model summary and graph structure...', end=' ')
    filename = args.output+'_summary.txt'
    with open(filename, 'w') as f:
        model.summary(print_fn=lambda x: f.write(x + '\n'))

    # Save the model dotfile
    plot_model(model, show_shapes=True, to_file=args.output+'_dot.png')
    print(u'\u2713')

    # Fit the model
    model = fit_model(model, tr_set, v_set, outputfile=args.output,
                      fine_tune_embedding=False)

    # Test set evaluation
    print('* Evaluate on test set...')
    model.load_weights(args.output+'_weights.h5')
    y_test = ts_set[1]
    y_pred = model.predict(ts_set[0]).ravel()

    # Plot non-normalized confusion matrix
    cnf_matrix = metrics.confusion_matrix(y_test, y_pred > 0.5)
    plt.figure(dpi=100)
    plot_confusion_matrix(cnf_matrix, classes=['METONLY', 'METX'],
                          title='Confusion matrix', cmap=plt.cm.Blues)
    plt.savefig(args.output+'_cm.png')

    # Save stats
    loss = metrics.log_loss(y_test, y_pred)
    acc = metrics.accuracy_score(y_test, y_pred > 0.5)
    prec = metrics.precision_score(y_test, y_pred > 0.5)
    rcll = metrics.recall_score(y_test, y_pred > 0.5)
    auc = metrics.roc_auc_score(y_test, y_pred)
    print('Test scores:\n * Log-Loss\t{:1.5f}\n * Accuracy:\t{:1.5f}\n '
          '* Precision:\t{:1.5f}\n * Recall:\t{:1.5f}\n * AUC:'
          '\t{:1.5f}'.format(loss, acc, prec, rcll, auc),
          file=open(args.output+'_stats.txt', 'w'))


################################################################################


if __name__ == '__main__':
    main()
