#!/usr/bin/env python
"""Cross-validate linear BOW logistic regression model.

Cross-validate a linear logistic regressione BOW model on the data extracted by
`extract_sequences.py` and matched by CEM (see `matching_step1.py` and
`matching_step2.R`).
"""
from __future__ import print_function

import argparse
import os
import warnings
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
import pandas as pd
from mbspbs10pc.plotting import plot_roc_curve
from mbspbs10pc.utils import load_data_labels, train_validation_test_split
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import PredefinedSplit


def parse_arguments():
    """"Parse input arguments."""
    parser = argparse.ArgumentParser(description='Bi-Timestamp-guided model '
                                                 'cross-validation.')
    parser.add_argument('-n', '--n_splits', type=int,
                        help='Number of re-shuffling & splitting iterations.',
                        default=5)
    parser.add_argument('-ng', '--ngram', type=int,
                        help='Max number of ngram allowed.',
                        default=1)
    parser.add_argument('-l', '--labels', type=str,
                        help='Path to the labels csv file generated by '
                             '`matching_step2.R`.',
                        default=None)
    parser.add_argument('-d', '--data', type=str,
                        help='Path to the data pkl file generated by '
                        '`extract_sequences.py`.',
                        default=None)
    parser.add_argument('-o', '--output', type=str,
                        help='Ouput file name root.',
                        default=None)
    args = parser.parse_args()
    return args


def init_main():
    """Initialize the main routine."""
    args = parse_arguments()

    if args.labels is None or not os.path.exists(args.labels):
        raise ValueError('{} labels file not found'.format(args.labels))

    if args.data is None or not os.path.exists(args.data):
        raise ValueError('{} data file not found'.format(args.data))

    # Check output filename
    if args.output is None:
        args.output = 'out_' + str(datetime.now())

    return args


def main():
    """Main train.py routine."""
    print('-------------------------------------------------------------------')
    print('MBS - PBS 10% dataset utility: cross_validate_linear_model.py')
    print('-------------------------------------------------------------------')
    args = init_main()

    print('\n* Number of splits: {}'.format(args.n_splits))
    print('* Max ngram: {}'.format(args.ngram))
    print('-------------------------------------------------------------------')

    # Load data
    print('\n* Loading {} and {}...'.format(args.data, args.labels), end=' ')
    dataset = load_data_labels(args.data, args.labels)
    print(u'\u2713')

    # Create BOW: fitting BOW before splitting is normally not recommended,
    # however in this case it can be accepted as the vocabulary size is limited
    # to the number of MBS items and it is very unlikely that
    print('* Preparing data...', end=' ')
    bow = CountVectorizer(ngram_range=(1, args.ngram),
                          tokenizer=lambda x: x.split(' '))
    xbow = bow.fit_transform(dataset['mbs_seq'])
    xbow = xbow / np.sum(xbow, axis=1)
    _dummy = np.empty((xbow.shape[0], 1))  # timestamps not used in this case
    print(u'\u2713')

    # Start to cross-validate the model
    cv_results_ = {'train_loss': [], 'test_loss': [],
                   'train_accuracy': [], 'test_accuracy': [],
                   'train_precision': [], 'test_precision': [],
                   'train_sensitivity': [], 'test_sensitivity': [],  # aka recall
                   'train_specificity': [], 'test_specificity': [],
                   'train_roc_auc': [], 'test_roc_auc': []}
    roc_curve = []

    print('\n* Cross-validation...')
    for k in range(args.n_splits):
        print('-------- SPLIT {} --------'.format(k))
        # Split in training, validation, test sets
        tr_set, v_set, ts_set = train_validation_test_split(
            [xbow, _dummy], dataset['Class'],
            test_size=0.4, validation_size=0.1,
            verbose=False, random_state0=k, random_state1=2*k)
        X_train, y_train = tr_set[0][0], tr_set[1]
        X_valid, y_valid = v_set[0][0], v_set[1]
        X_test, y_test = ts_set[0][0], ts_set[1]

        # Merge back train and validation and use PredefinedSplit to optimize C
        # on the validation set only (this improves consistency with keras
        # that uses the validation set for early stopping)
        X_learn = sp.sparse.vstack((X_train, X_valid))
        y_learn = np.hstack((y_train, y_valid))
        # keep track of where training and validation set are in the
        # reconstructed learning set
        test_fold = np.array([0]*X_train.shape[0] + [1]*X_valid.shape[0])

        # Define and fit the cross-validated learning model
        model = LogisticRegressionCV(Cs=np.logspace(0, 5, 10),
                                     penalty='l2',
                                     solver='saga',
                                     cv=PredefinedSplit(test_fold),
                                     n_jobs=-1)
        model.fit(X_learn, y_learn)

        # Re-evaluate on training set
        print('* Re-evaluating on training set... ', end='')
        y_pred_learn = model.predict_proba(X_learn)[:, 1]
        print(u'\u2713')

        # Test set evaluation
        print('* Evaluating on test set... ', end='')
        y_pred = model.predict_proba(X_test)[:, 1]
        print(u'\u2713')

        # Evaluate test stats
        cv_results_['test_loss'].append(
            metrics.log_loss(y_test, y_pred))
        cv_results_['test_accuracy'].append(
            metrics.accuracy_score(y_test, y_pred > 0.5))
        cv_results_['test_precision'].append(
            metrics.precision_score(y_test, y_pred > 0.5))
        cv_results_['test_sensitivity'].append(  # aka recall
            metrics.recall_score(y_test, y_pred > 0.5, pos_label=1))
        cv_results_['test_specificity'].append(
            metrics.recall_score(y_test, y_pred > 0.5, pos_label=0))
        cv_results_['test_roc_auc'].append(
            metrics.roc_auc_score(y_test, y_pred))
        roc_curve.append(metrics.roc_curve(y_test, y_pred))

        # Evaluate train stats
        cv_results_['train_loss'].append(
            metrics.log_loss(y_learn, y_pred_learn))
        cv_results_['train_accuracy'].append(
            metrics.accuracy_score(y_learn, y_pred_learn > 0.5))
        cv_results_['train_precision'].append(
            metrics.precision_score(y_learn, y_pred_learn > 0.5))
        cv_results_['train_sensitivity'].append(  # aka recall
            metrics.recall_score(y_learn, y_pred_learn > 0.5, pos_label=1))
        cv_results_['train_specificity'].append(
            metrics.recall_score(y_learn, y_pred_learn > 0.5, pos_label=0))
        cv_results_['train_roc_auc'].append(
            metrics.roc_auc_score(y_learn, y_pred_learn))

    print('* Saving cross-validation results... ', end='')
    cv_results_ = pd.DataFrame.from_dict(cv_results_, orient='index')
    cols = cv_results_.columns
    cv_results_['mean'] = cv_results_[cols].mean(axis=1)
    cv_results_['std'] = cv_results_[cols].std(axis=1)
    cv_results_.to_csv(args.output+'_cv_results.csv')
    print(u'\u2713')
    print(cv_results_)

    # Plot ROC curve
    print('Generating ROC curves... ', end='')
    plt.figure(dpi=100)
    fpr = [roc[0] for roc in roc_curve]
    tpr = [roc[1] for roc in roc_curve]
    plot_roc_curve(fpr, tpr, cv_results_.loc['test_roc_auc', cols])
    plt.savefig(args.output + '_roc.png')
    print(u'\u2713')










################################################################################


if __name__ == '__main__':
    warnings.filterwarnings('ignore')
    main()
