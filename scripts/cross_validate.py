#!/usr/bin/env python
"""Cross-validate bidirectional timestamp-guided attention model.

Cross-validate the keras model on the data extracted by `extract_sequences.py`
and matched by CEM (see `matching_step1.py` and `matching_step2.R`).
"""

from __future__ import print_function

import argparse
import os
import warnings
from datetime import datetime

import matplotlib.pyplot as plt
import pandas as pd
from keras import optimizers as opt
from keras.layers import CuDNNLSTM
from keras.utils import plot_model
from mbspbs10pc.model import build_model
from mbspbs10pc.plotting import plot_history
from mbspbs10pc.utils import (load_data_labels, tokenize,
                              train_validation_test_split)
from mbspbs10pc.fit_utils import (concatenate_history, get_callbacks)
from sklearn import metrics


def parse_arguments():
    """"Parse input arguments."""
    parser = argparse.ArgumentParser(description='Bi-Timestamp-guided model '
                                                 'cross-validation.')
    parser.add_argument('-n', '--n_splits', type=int,
                        help='Number of re-shuffling & splitting iterations.',
                        default=5)
    parser.add_argument('-l', '--labels', type=str,
                        help='Path to the labels csv file generated by '
                             '`matching_step2.R`.',
                        default=None)
    parser.add_argument('-d', '--data', type=str,
                        help='Path to the data pkl file generated by '
                        '`extract_sequences.py`.',
                        default=None)
    parser.add_argument('-e', '--embedding', type=str,
                        help='Path to the embedding matrix csv file.',
                        default=None)
    parser.add_argument('-o', '--output', type=str,
                        help='Ouput file name root.',
                        default=None)
    args = parser.parse_args()
    return args


def init_main():
    """Initialize the main routine."""
    args = parse_arguments()

    if args.labels is None or not os.path.exists(args.labels):
        raise ValueError('{} labels file not found'.format(args.labels))

    if args.data is None or not os.path.exists(args.data):
        raise ValueError('{} data file not found'.format(args.data))

    # Check output filename
    if args.output is None:
        args.output = 'out_' + str(datetime.now())

    return args


def fit_model(model, training_set, validation_set, outputfile,
              fine_tune_embedding=False):
    # Start training
    callbacks = get_callbacks(RLRP_patience=3, ES_patience=12,
                              MC_filepath=outputfile)

    history = model.fit(x=training_set[0], y=training_set[1],
                        epochs=100,
                        callbacks=callbacks,
                        batch_size=128,
                        validation_data=validation_set)

    if fine_tune_embedding:
        print(print('* Fine-tuning the embedding layer...'))
        # Fine-tune the embedding layers
        model.get_layer('mbs_embedding').trainable = True

        # Re-compile the model
        model.compile(optimizer=opt.RMSprop(lr=0.001),
                      loss='binary_crossentropy',
                      metrics=['acc'])

        callbacks = get_callbacks(RLRP_patience=5, ES_patience=10,
                                  MC_filepath=outputfile + '_finetuned')

        history_ft = model.fit(x=training_set[0], y=training_set[1],
                               epochs=200,
                               callbacks=callbacks,
                               batch_size=128,
                               validation_data=validation_set,
                               initial_epoch=history.epoch[-1])
        history = concatenate_history([history, history_ft])

    print('* Saving training history...', end=' ')
    plt.figure(dpi=100)
    plot_history(history)
    plt.savefig(outputfile + '_loss_history.png')
    print(u'\u2713')
    return model


def main():
    """Main train.py routine."""
    print('-------------------------------------------------------------------')
    print('MBS - PBS 10% dataset utility: cross_validate.py')
    print('-------------------------------------------------------------------')
    args = init_main()

    print('\n* Number of splits: {}'.format(args.n_splits))
    print('-------------------------------------------------------------------')

    # Load data
    print('* Loading {} and {}...'.format(args.data, args.labels), end=' ')
    dataset = load_data_labels(args.data, args.labels)
    print(u'\u2713')

    # Load embedding matrix
    print('* Loading {}...'.format(args.embedding), end=' ')
    embedding_matrix = pd.read_csv(
        args.embedding, header=0, index_col=0).values
    print(u'\u2713')

    # Tokenize and pad
    print('* Preparing data...', end=' ')
    padded_mbs_seq, padded_timestamp_seq, _ = tokenize(dataset)
    maxlen = padded_mbs_seq.shape[1]

    # Build the model
    print('* Building model...', end=' ')
    model = build_model(mbs_input_shape=(maxlen,),
                        timestamp_input_shape=(maxlen, 1),
                        vocabulary_size=embedding_matrix.shape[0],
                        embedding_size=embedding_matrix.shape[1],
                        recurrent_units=64,
                        dense_units=64,
                        bidirectional=True,
                        LSTMLayer=CuDNNLSTM)

    # Initialize the embedding matrix
    model.get_layer('mbs_embedding').set_weights([embedding_matrix])
    model.get_layer('mbs_embedding').trainable = True

    # Compile the model
    model.compile(optimizer=opt.RMSprop(lr=0.004),
                  loss='binary_crossentropy',
                  metrics=['acc'])
    print(u'\u2713')

    # Print the summary to file
    print('* Saving model summary and graph structure...', end=' ')
    filename = args.output + '_summary.txt'
    with open(filename, 'w') as f:
        model.summary(print_fn=lambda x: f.write(x + '\n'))

    # Save the model dotfile
    plot_model(model, show_shapes=True, to_file=args.output+'_dot.png')
    print(u'\u2713')

    # Start to cross-validate the model
    cv_results_ = {'train_loss': [], 'test_loss': [],
                   'train_accuracy': [], 'test_accuracy': [],
                   'train_precision': [], 'test_precision': [],
                   'train_recall': [], 'test_recall': [],
                   'train_roc_auc': [], 'test_roc_auc': []}

    print('* Cross-validation...')
    for k in range(args.n_splits):
        # Split in training, validation, test sets
        tr_set, v_set, ts_set = train_validation_test_split(
            [padded_mbs_seq, padded_timestamp_seq], dataset['Class'],
            test_size=0.4, validation_size=0.1,
            verbose=False, random_state0=k, random_state1=2*k)

        # Fit the model
        print('[{}] Training model...'.format(k))
        model = fit_model(model, tr_set, v_set, outputfile=args.output+str(k),
                          fine_tune_embedding=False)

        # Re-evaluate on training set
        print('* Re-evaluate on test set...')
        y_train = tr_set[1]
        y_pred_train = model.predict(tr_set[0]).ravel()

        # Test set evaluation
        print('* Evaluate on test set...')
        y_test = ts_set[1]
        y_pred = model.predict(ts_set[0]).ravel()

        # Evaluate test stats
        cv_results_['test_loss'].append(
            metrics.log_loss(y_test, y_pred))
        cv_results_['test_accuracy'].append(
            metrics.accuracy_score(y_test, y_pred > 0.5))
        cv_results_['test_precision'].append(
            metrics.precision_score(y_test, y_pred > 0.5))
        cv_results_['test_recall'].append(
            metrics.recall_score(y_test, y_pred > 0.5))
        cv_results_['test_roc_auc'].append(
            metrics.roc_auc_score(y_test, y_pred))

        # Evaluate train stats
        cv_results_['train_loss'].append(
            metrics.log_loss(y_train, y_pred_train))
        cv_results_['train_accuracy'].append(
            metrics.accuracy_score(y_train, y_pred_train > 0.5))
        cv_results_['train_precision'].append(
            metrics.precision_score(y_train, y_pred_train > 0.5))
        cv_results_['train_recall'].append(
            metrics.recall_score(y_train, y_pred_train > 0.5))
        cv_results_['train_roc_auc'].append(
            metrics.roc_auc_score(y_train, y_pred_train))

    print('* Saving cross-validation results...', end='')
    cv_results_ = pd.DataFrame.from_dict(cv_results_, orient='index')
    cols = cv_results_.columns
    cv_results_['mean'] = cv_results_[cols].mean(axis=1)
    cv_results_['std'] = cv_results_[cols].std(axis=1)
    cv_results_.to_csv(args.output+'_cv_results.csv')
    print(u'\u2713')
    print(cv_results_)


################################################################################


if __name__ == '__main__':
    warnings.filterwarnings('ignore')
    main()
