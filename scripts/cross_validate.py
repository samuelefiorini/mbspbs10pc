#!/usr/bin/env python
"""Cross-validate bidirectional timestamp-guided attention model.

Cross-validate the keras model on the data extracted by `extract_sequences.py`
and matched by CEM (see `matching_step1.py` and `matching_step2.R`).
"""

from __future__ import print_function

import argparse
import os
import warnings
from datetime import datetime

import matplotlib.pyplot as plt
import pandas as pd
from keras import backend as K
from keras import optimizers as opt
from keras.layers import CuDNNLSTM as LSTM
from mbspbs10pc.fit_utils import concatenate_history, get_callbacks
from mbspbs10pc.model import (__implemeted_models__, build_attention_model,
                              build_baseline_model, build_model)
from mbspbs10pc.plotting import plot_history, plot_roc_curve
from mbspbs10pc.utils import (load_data_labels, tokenize,
                              train_validation_test_split)
from sklearn import metrics


def parse_arguments():
    """"Parse input arguments."""
    parser = argparse.ArgumentParser(description='Bi-Timestamp-guided model '
                                                 'cross-validation.')
    parser.add_argument('-n', '--n_splits', type=int,
                        help='Number of re-shuffling & splitting iterations.',
                        default=5)
    parser.add_argument('-l', '--labels', type=str,
                        help='Path to the labels csv file generated by '
                             '`matching_step2.R`.',
                        default=None)
    parser.add_argument('-d', '--data', type=str,
                        help='Path to the data pkl file generated by '
                        '`extract_sequences.py`.',
                        default=None)
    parser.add_argument('-e', '--embedding', type=str,
                        help='Path to the embedding matrix csv file.',
                        default=None)
    parser.add_argument('-m', '--model', type=str,
                        help="""Which model should be used. Argument must be
                        either: 'baseline' for Embedding + LSTM only
                        , 'attention' for Embedding + LSTM with standard
                        attention or 'proposed' for bidirectional
                        timestamp-guided attention model (default)""",
                        default='proposed')
    parser.add_argument('-o', '--output', type=str,
                        help='Ouput file name root.',
                        default=None)
    args = parser.parse_args()
    return args


def init_main():
    """Initialize the main routine."""
    args = parse_arguments()

    if args.labels is None or not os.path.exists(args.labels):
        raise ValueError('{} labels file not found'.format(args.labels))

    if args.data is None or not os.path.exists(args.data):
        raise ValueError('{} data file not found'.format(args.data))

    # Check output filename
    if args.output is None:
        args.output = 'out_' + str(datetime.now())

    # Check model architecture
    if args.model not in __implemeted_models__:
        raise ValueError('model must be in {}. '
                         'Found: {}.'.format(__implemeted_models__, args.model))

    return args


def fit_model(model, training_set, validation_set, outputfile,
              fine_tune_embedding=False):
    # Start training
    callbacks = get_callbacks(RLRP_patience=3, ES_patience=9,
                              MC_filepath=outputfile)

    history = model.fit(x=training_set[0], y=training_set[1],
                        epochs=100,
                        callbacks=callbacks,
                        batch_size=128,
                        validation_data=validation_set)

    if fine_tune_embedding:
        print(print('* Fine-tuning the embedding layer...'))
        # Fine-tune the embedding layers
        model.get_layer('mbs_embedding').trainable = True

        # Re-compile the model
        model.compile(optimizer=opt.RMSprop(lr=0.001),
                      loss='binary_crossentropy',
                      metrics=['acc'])

        callbacks = get_callbacks(RLRP_patience=3, ES_patience=9,
                                  MC_filepath=outputfile + '_finetuned')

        history_ft = model.fit(x=training_set[0], y=training_set[1],
                               epochs=200,
                               callbacks=callbacks,
                               batch_size=128,
                               validation_data=validation_set,
                               initial_epoch=history.epoch[-1])
        history = concatenate_history([history, history_ft])

    print('* Saving training history...', end=' ')
    plt.figure(dpi=100)
    plot_history(history)
    plt.savefig(outputfile + '_loss_history.png')
    print(u'\u2713')
    return model


def main():
    """Main train.py routine."""
    print('-------------------------------------------------------------------')
    print('MBS - PBS 10% dataset utility: cross_validate.py')
    print('-------------------------------------------------------------------')
    args = init_main()

    print('\n* Selected model: {}'.format(args.model))
    print('* Number of splits: {}'.format(args.n_splits))
    print('* {} embedding initialization'.format('GloVe' if args.embedding is not None else 'Random'))
    print('-------------------------------------------------------------------')

    # Load data
    print('* Loading {} and {}...'.format(args.data, args.labels), end=' ')
    dataset = load_data_labels(args.data, args.labels)
    print(u'\u2713')

    # Tokenize and pad
    print('* Preparing data...', end=' ')
    padded_mbs_seq, padded_timestamp_seq, tokenizer = tokenize(dataset)
    maxlen = padded_mbs_seq.shape[1]
    vocabulary_size = len(tokenizer.word_index.keys()) + 1  # add the 0 row

    # Load embedding matrix
    if args.embedding is not None:
        print('* Loading {}...'.format(args.embedding), end=' ')
        embedding_matrix = pd.read_csv(
            args.embedding, header=0, index_col=0).values
        embedding_size = embedding_matrix.shape[1]
        print(u'\u2713')
    else:
        embedding_size = 50 # default size


    # Define the model arguments
    kwargs = {'mbs_input_shape': (maxlen,),
              'timestamp_input_shape': (maxlen, 1),
              'vocabulary_size': vocabulary_size,
              'embedding_size': embedding_size,
              'recurrent_units': 32,
              'dense_units': 32,
              'bidirectional': True,
              'LSTMLayer': LSTM}

    # Start to cross-validate the model
    cv_results_ = {'train_loss': [], 'test_loss': [],
                   'train_accuracy': [], 'test_accuracy': [],
                   'train_precision': [], 'test_precision': [],
                   'train_sensitivity': [], 'test_sensitivity': [],  # aka recall
                   'train_specificity': [], 'test_specificity': [],
                   'train_roc_auc': [], 'test_roc_auc': []}
    roc_curve = []

    print('* Cross-validation...')
    for k in range(args.n_splits):
        print('-------- SPLIT {} --------'.format(k))
        # Split in training, validation, test sets
        tr_set, v_set, ts_set = train_validation_test_split(
            [padded_mbs_seq, padded_timestamp_seq], dataset['Class'],
            test_size=0.4, validation_size=0.1,
            verbose=False, random_state0=k, random_state1=2*k)

        # Drop the timestamps when not needed
        if args.model.lower() in ['baseline', 'attention']:
            tr_set = (tr_set[0][0], tr_set[1])
            v_set = (v_set[0][0], v_set[1])
            ts_set = (ts_set[0][0], ts_set[1])

        # Build the model
        print('* Building model...', end=' ')
        # Baseline model
        if args.model.lower() == 'baseline':
            model = build_baseline_model(**kwargs)
        elif args.model.lower() == 'attention':
            model = build_attention_model(**kwargs)
        elif args.model.lower() == 'proposed':
            model = build_model(**kwargs)

        # Initialize the embedding matrix
        if args.embedding is not None:
            model.get_layer('mbs_embedding').set_weights([embedding_matrix])
            model.get_layer('mbs_embedding').trainable = True

        # Compile the model
        model.compile(optimizer=opt.RMSprop(lr=0.004),
                      loss='binary_crossentropy',
                      metrics=['acc'])
        print(u'\u2713')

        # Fit the model
        print('[{}] Training model...'.format(k))
        model = fit_model(model, tr_set, v_set, outputfile=args.output+str(k),
                          fine_tune_embedding=False)

        # Re-evaluate on training set
        model.load_weights(args.output+str(k)+'_weights.h5')
        print('* Re-evaluating on training set... ', end='')
        y_train = tr_set[1]
        y_pred_train = model.predict(tr_set[0]).ravel()
        print(u'\u2713')

        # Test set evaluation
        print('* Evaluating on test set... ', end='')
        y_test = ts_set[1]
        y_pred = model.predict(ts_set[0]).ravel()
        print(u'\u2713')

        # Evaluate test stats
        cv_results_['test_loss'].append(
            metrics.log_loss(y_test, y_pred))
        cv_results_['test_accuracy'].append(
            metrics.accuracy_score(y_test, y_pred > 0.5))
        cv_results_['test_precision'].append(
            metrics.precision_score(y_test, y_pred > 0.5))
        cv_results_['test_sensitivity'].append(  # aka recall
            metrics.recall_score(y_test, y_pred > 0.5, pos_label=1))
        cv_results_['test_specificity'].append(
            metrics.recall_score(y_test, y_pred > 0.5, pos_label=0))
        cv_results_['test_roc_auc'].append(
            metrics.roc_auc_score(y_test, y_pred))
        roc_curve.append(metrics.roc_curve(y_test, y_pred))

        # Evaluate train stats
        cv_results_['train_loss'].append(
            metrics.log_loss(y_train, y_pred_train))
        cv_results_['train_accuracy'].append(
            metrics.accuracy_score(y_train, y_pred_train > 0.5))
        cv_results_['train_precision'].append(
            metrics.precision_score(y_train, y_pred_train > 0.5))
        cv_results_['train_sensitivity'].append(  # aka recall
            metrics.recall_score(y_train, y_pred_train > 0.5, pos_label=1))
        cv_results_['train_specificity'].append(
            metrics.recall_score(y_train, y_pred_train > 0.5, pos_label=0))
        cv_results_['train_roc_auc'].append(
            metrics.roc_auc_score(y_train, y_pred_train))

        print('* Clearing session... ', end='')
        del model
        K.clear_session()
        print(u'\u2713')

    print('* Saving cross-validation results... ', end='')
    cv_results_ = pd.DataFrame.from_dict(cv_results_, orient='index')
    cols = cv_results_.columns
    cv_results_['mean'] = cv_results_[cols].mean(axis=1)
    cv_results_['std'] = cv_results_[cols].std(axis=1)
    cv_results_.to_csv(args.output+'_cv_results.csv')
    print(u'\u2713')
    print(cv_results_)

    # Plot ROC curve
    print('Generating ROC curves... ', end='')
    plt.figure(dpi=100)
    fpr = [roc[0] for roc in roc_curve]
    tpr = [roc[1] for roc in roc_curve]
    plot_roc_curve(fpr, tpr, cv_results_.loc['test_roc_auc', cols])
    plt.savefig(args.output + '_roc.png')
    print(u'\u2713')


################################################################################


if __name__ == '__main__':
    warnings.filterwarnings('ignore')
    main()
